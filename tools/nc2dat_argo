#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
#  Qing Li, 20180322

import argparse
import sys
import numpy as np
import pandas as pd
import datetime
from netCDF4 import Dataset, date2num, num2date
from gotmtool import nctime_to_datetime, nctime_indices, write_pfl

def main():
    # process the input arguments
    parser = argparse.ArgumentParser(description="""
            Read the Argo profile data in netCDF format and
            output the temperature and salinity profiles in a text
            file in the GOTM input file format.
            The argument -r RANGE sets the searching radius in degrees.
            The argument -dr DATERANGE sets the searching date range in days.
              """)
    parser.add_argument('-i', '--input', action='store', dest='fname_in',
            metavar='NCFILENAME', required=True, help='Input netCDF filename')
    parser.add_argument('-lat', '--latitude', action='store', dest='lat',
            metavar='LATITUDE', required=True,
            help='Latitude of the requested location (-85, 85)')
    parser.add_argument('-lon', '--longitude', action='store', dest='lon',
            metavar='LONGITUDE', required=True,
            help='Longitude of the requested location (-180, 360)')
    parser.add_argument('-r', '--range', action='store', dest='range',
            metavar='RANGE', required=True,
            help='Range in degrees of the searching radius')
    parser.add_argument('-d', '--date', action='store', dest='date',
            metavar='DATE', required=True,
            help='The target date of search, in the format of YYYYMMDD')
    parser.add_argument('-dr', '--date_range', action='store', dest='date_range',
            metavar='DATERANGE',
            help='Date range in days for searching the initial profiles before or after the starting date')
    parser.add_argument('-ot', '--output_temp', action='store',
            dest='fname_out_temp', metavar='TEMPFILENAME',
            help='Output filename for temperature profile')
    parser.add_argument('-os', '--output_salt', action='store',
            dest='fname_out_salt', metavar='SALTFILENAME',
            help='Output filename for salinity profile')
    parser.add_argument('-maxd', '--max_depth', action='store',
            dest='max_depth', metavar='MAXDEPTH',
            help='Max depth of the profiles')
    parser.add_argument('-iy', '--ignore_year', action='store_true',
            dest='l_ignore_year',
            help='Ignore the year when searching profiles within the time range')
    parser.add_argument('-dry', '--dry', action='store_true', dest='l_dry_run',
            help='Check the available profiles without outputting them into files')
    parser.add_argument('--version', action='version', version='%(prog)s: 1.1')
    # parsing arguments and save to args
    args=parser.parse_args()

    # check arguments
    fname_in = args.fname_in
    rlat = float(args.lat)
    rlon = float(args.lon)
    dr = float(args.range)
    l_ignore_year = args.l_ignore_year

    date_base = args.date
    date_range = float(args.date_range)

    if rlon >= 180:
        rlon = rlon - 360.0

    if rlat > 85.0 or rlat < -85.0 or rlon > 180.0 or rlon < -180.0:
        parser.print_help()
        sys.exit(1)

    if args.fname_out_temp:
        fname_out_temp = args.fname_out_temp
    else:
        fname_out_temp = 'tprof_file.dat'

    if args.fname_out_salt:
        fname_out_salt = args.fname_out_salt
    else:
        fname_out_salt = 'sprof_file.dat'

    if args.max_depth:
        max_depth = float(args.max_depth)
    else:
        max_depth = 500.0

    # read data
    infile = Dataset(fname_in, 'r')
    attlist = infile.ncattrs()

    # read latitude and longitude
    lat = infile.variables['Latitude'][:]
    lon = infile.variables['Longitude'][:]

    # find the indices of points within the search range
    idx_loc = np.where(np.all([lat>=rlat-dr, lat<rlat+dr, lon>=rlon-dr,
        lon<rlon+dr], axis=0))[0]

    # check if any profiles are found
    if idx_loc.size == 0:
        # no message if dry run
        if not args.l_dry_run:
            print("Argo profile at location [lon={}, lat={}] not available. Skip.".format(rlon, rlat))
        sys.exit(1)

    # read time
    nctime = infile.variables['time']
    dttime = nctime_to_datetime(nctime, tidx_arr=idx_loc)

    # read cast number to get the indices in the full record
    cast = infile.variables['cast'][idx_loc]

    # pandas Series data type
    cast_series = pd.Series(cast, index=dttime)

    # get starting and ending indices
    dt_base = pd.to_datetime(date_base)

    if l_ignore_year:
        ystart = np.int(cast_series.index.min().strftime('%Y'))
        yend   = np.int(cast_series.index.max().strftime('%Y'))
        idx_cast_list = []
        for yyyy in np.arange(ystart, yend+1):
            dt_tmp = dt_base.replace(year=yyyy)
            dt_start = dt_tmp - pd.DateOffset(date_range)
            dt_end   = dt_tmp + pd.DateOffset(date_range)
            idx_cast_tmp = cast_series[dt_start:dt_end]
            if not idx_cast_tmp.empty:
                idx_cast_list.extend(idx_cast_tmp.values.tolist())
        idx_cast = np.array(idx_cast_list)-1
    else:
        dt_start = dt_base - pd.DateOffset(date_range)
        dt_end   = dt_base + pd.DateOffset(date_range)
        idx_cast = cast_series[dt_start:dt_end].values-1

    # check if any profiles are found
    if idx_cast.size == 0:
        # no message if dry run
        if not args.l_dry_run:
            print("Argo profile at time {} not available. Skip.".format(date_base))
        sys.exit(1)

    # read depth (limited to upper max_depth)
    depth = infile.variables['depth'][:]
    depth = abs(depth)
    idx_depth_max = np.searchsorted(depth, max_depth, side='left')+1
    ddat = -depth[:idx_depth_max]

    # get global missing value
    if 'missing_value' in attlist:
        gmvalue = infile.missing_value
    elif '_FillValue' in attlist:
        gmvalue = infile._FillValue
    else:
        # turn off auto mask, handled in write_pfl()
        infile.set_auto_mask(False)
        gmvalue = np.nan

    # read temperature and salinity
    np.warnings.filterwarnings('ignore')
    temp = infile.variables['Temperature'][idx_cast,:idx_depth_max]
    temp_qcf = infile.variables['TempFlag'][idx_cast,:idx_depth_max]
    temp[temp_qcf>0] = gmvalue
    salt = infile.variables['Salinity'][idx_cast,:idx_depth_max]
    salt_qcf = infile.variables['SalnFlag'][idx_cast,:idx_depth_max]
    salt[temp_qcf>0] = gmvalue

    # remove profiles with too many missing values near the surface
    # check the upper 100 m of the profile
    idx_depth_sl = np.searchsorted(depth, 100, side='left')
    temp_sl = temp[:,:idx_depth_sl+1]
    temp_sl[temp_sl==gmvalue] = np.NaN
    # number of profiles
    npfl = temp_sl.shape[0]
    # number of levels
    nlev = temp_sl.shape[1]
    # indices of profiles that have enough valid data
    idx_pass = []
    for i in np.arange(npfl):
        # number of NaNs in the profile
        nnan = np.count_nonzero(np.isnan(temp_sl[i,:]))
        if nnan < nlev/3:
            # ignore the profile if the number of missing value is greater
            # than 1/3 of the total value within the upper 100 m
            idx_pass.append(i)

    # skip if no valid profiles
    if len(idx_pass) < 1:
        # no message if dry run
        if not args.l_dry_run:
            print("Too many missing values in the profiles. Skip.")
        sys.exit(1)

    # updated indices in the full record
    idx_cast_new = idx_cast[idx_pass]

    # updated temperature and salinity profiles
    temp_avail = temp[idx_pass, :]
    salt_avail = salt[idx_pass, :]

    # write in file
    # select the profile which has the median of near surface temperature
    # (averaged temperature between -10 m and -50 m)
    idx_depth_avg1 = np.searchsorted(depth, 10, side='left')
    idx_depth_avg2 = np.searchsorted(depth, 50, side='left')
    temp_smp = temp_avail[:,idx_depth_avg1:idx_depth_avg2+1]
    temp_smp[temp_smp==gmvalue] = np.NaN
    temp_avg = np.nanmean(temp_smp, axis=1)
    # find the index of the median
    idx_temp_median = np.argsort(temp_avg)[(len(temp_avg)-1)//2]
    # set the date to the starting date
    tdat_sgl = dt_base
    # write profiles
    if args.l_dry_run:
        # print("Number of profiles: {}".format(len(temp_avg)))
        # print("Date of output profile: {}".format(tdat[idx_temp_median]))
        # print("Longitude: {}".format(lon[idx_cast[idx_temp_median]]))
        # print("Latitude: {}".format(lat[idx_cast[idx_temp_median]]))
        tmdn = nctime_to_datetime(nctime, tidx_arr=idx_cast_new[idx_temp_median])
        print("{:} {:6.2f} {:6.2f} {:3d} {:6.2f} {:6.2f} {:6.2f}".format(tmdn.strftime('%Y-%m-%d %H:%M:%S'),
            lon[idx_cast_new[idx_temp_median]], lat[idx_cast_new[idx_temp_median]],
            len(temp_avg), rlon, rlat, temp_avg[idx_temp_median]))
    else:
        write_pfl(fname_out_temp, [tdat_sgl], ddat, [temp_avail[[idx_temp_median],:]], mask=gmvalue)
        write_pfl(fname_out_salt, [tdat_sgl], ddat, [salt_avail[[idx_temp_median],:]], mask=gmvalue)

if __name__ == "__main__":
    main()
